# Topic 2 Exercises: Linear Regression


## Computing

### 3.6

3.6.1 
```{r}
library(MASS)
library(ISLR)
```

3.6.2
```{r}
fix(Boston)
names(Boston)
attach(Boston)
lm.fit=lm(medv~lstat)
summary(lm.fit)
names(lm.fit)
confint(lm.fit)
predict(lm.fit, data.frame(lstat=(c(5,10,15))), interval="confidence")
predict(lm.fit, data.frame(lstat=(c(5,10,15))), interval="prediction")
plot(lstat, medv)
abline(lm.fit)
abline(lm.fit, lwd=3)
abline(lm.fit, lwd=3, col="red")
plot(lstat, medv, col="red")
plot(lstat, medv, pch=20)
plot(lstat, medv, pch="+")
plot(1:20, 1:20, pch=1:20)
par(mfrow=c(2,2))
plot(lm.fit)
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
plot(hatvalues(lm.fit))
which.max(hatvalues(lm.fit))
```

3.6.3
```{r}
lm.fit=lm(medv~lstat+age, data=Boston)
summary(lm.fit)

lm.fit=lm(medv~.,data=Boston)
summary(lm.fit)

library(car)
vif(lm.fit)
lm.fit1=lm(medv~.-age,data=Boston)
summary(lm.fit1)
lm.fit1=update(lm.fit,~.-age)
```

3.6.4
```{r}
summary(lm(medv~lstat*age,data=Boston))
```

3.6.5
```{r}
lm.fit2=lm(medv~lstat+I(lstat^2))
summary(lm.fit2)
lm.fit=lm(medv~lstat)
anova(lm.fit, lm.fit2)

par(mfrow=c(2,2))
plot(lm.fit2)
lm.fit5=lm(medv~poly(lstat,5))
summary(lm.fit5)

summary(lm(medv~log(rm), data=Boston))
```

3.6.6

```{r}
fix(Carseats)
names(Carseats)

lm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)
attach(Carseats)
contrasts(ShelveLoc)
```

3.6.7

```{r}
LoadLibraries = function(){
  library(ISLR)
  library(MASS)
  print("The libraries have been loaded.")
}
LoadLibraries
LoadLibraries()
```


### Problem 13
a)
```{r}
set.seed(1)
x = rnorm(100)
```
b)
```{r}
eps = rnorm(100, mean=0, sd=sqrt(0.25))
```
c)
```{r}
y = -1 + 0.5*x + eps
length(y)
```
The length of the vector is 100. The value of $\beta_0$ is -1 and the value of $\beta_1$is 0.5.

d)
```{r}
plot(x,y)
```
I observe a positive relationship between x and y from the plot.s

```{r}
model = lm(y~x)
summary(model)
```
From the summary of the model, we can see that $\hat\beta_0 = -1.01885$ which is close to our previous of $\beta_0 = -1$ and $\hat\beta_1 = 0.49947$ which is close to our previous value of $\beta_1 = 0.5$.

f)
```{r}
plot(x,y)
abline(model)
```
g)
```{r}
polynomial_model = lm(y~x+I(x^2))
anova(model, polynomial_model)
```
We cannot say that this model is any better than the previous one, given it's high p-value.

h)
```{r}
set.seed(1)
x = rnorm(100)
eps_less = rnorm(100, mean=0, sd=sqrt(0.125))
y = -1 + 0.5*x + eps_less
model_less_noise = lm(y~x)
summary(model_less_noise)
length(y)
plot(x,y)
abline(model_less_noise)
```
The model is better because of the smaller error in the eps variable.

i)
```{r}
set.seed(1)
x = rnorm(100)
eps_more = rnorm(100, mean=0, sd=sqrt(0.5))
y = -1 + 0.5*x + eps_more
model_more_noise = lm(y~x)
summary(model_more_noise)
length(y)
plot(x,y)
abline(model_more_noise)
```
The model is not as good at predicting because of the large error on the data.

j)
```{r}
confint(model_less_noise)
confint(model)
confint(model_more_noise)
```

More noise leads to a larger range in the confidence interval.

## Theory

### Reading Questions

p.66)

We can see in figure 3.1 that there is a correlation between error and variance. The error bars are larger as the value of TV grows.

p.77)

It won't be possible to fit a multiple regression model because if p>n, there are more coefficients to estimate than observations from which to estimate them.

### 3.7.3

a)

Given that the values of IQ and GPA are fixed, we shouldn't pay attention to these coefficients. By looking at coefficient 3, we can see that females make more than males However, because of coefficient 5, we see that we must take the gpa into account. Therefore, iv) is true.

b)

```{r}
50 + 4.0*20 + 110*0.07 + 1*35 + 4.0*110*0.01 + 1*4.0*(-10)
```

The predicted starting salary for this person would be 137.1 thousdans of dollars.

c)

The coefficient's magnitude is not enough to determine how large or small the interaction between GPA and IQ is. In order to prove that there is a large interaction effect, we must compare this model to another model without this interaction.

### 3.7.4

a)

The RSS on the cubic model would be lower because the higher the flexiblity, the lower the RSS for the training data.

b)

Although the cubic model will give a lower RSS for the training data, it will probably give more errors with the test data given that the true relationship between x and y is linear. Therefore, the linear model would give less errors for the test data.

c)

Given that the cubic regression model is more flexible, we would expect a lower RSS for the cubic model still.

d)

We would need to know the exact relationship between x and y to know whether the linear or cubic model will give more error. However, if the data is not exactly linear, the cubic model will probably give less error.

d)
